#Analysis for paper was conducted with DADA2 v1.10

###Installing DADA2 for the first time? Follow the tutorial here: https://benjjneb.github.io/dada2/dada-installation.html

library("devtools")
devtools::install_github("benjjneb/dada2", ref = "v1.10") #installs dada2 version 1.10 (most recent version as of 4/1/2019)
library("dada2") #load dada2 package
packageVersion("dada2") #Version should be v1.10

###Sequence data is uploaded to NCBI Sequence Read Archive under accession number: PRJNA578400

###Read in files
path <- "~/Documents/SequencingData/LibraryY_TideExpt/fastq_files_all" #Change path depending on where your sequence files are
list.files(path) #files should be in the format: Sample_number_L001_R[1or2]_001.fastq.gz

#read in names of gzipped fastq files
fnFs <- sort(list.files(path, pattern = "_R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq.gz", full.names = TRUE))

#extract sample names (may need to be changed depending on filename format). Mine output as "Sample-#"
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

### Inspect Quality of Reads:
plotQualityProfile(fnFs[9:10]) #this checks quality of two of the seawater samples
plotQualityProfile(fnRs[9:10]) #Check reverse reads too

#Green is median quality an dorange is the quartiles of quality distribution. 
#Forward reads: Trimming the last few nucleotides is advised according to the DADA2 tutorial. 
#I will do as they do in the tutorial and truncate the last 10 nucleotides (at position 240).
#Reverse reads: Generally exhibit poorer quality.I trimmed around 190, where the quality really starts to decrease

### Filter and trim the reads
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen = c(240, 200), maxN = 0, maxEE = c(2,2), rm.phix = TRUE, compress = TRUE, multithread = TRUE) 
head(out)
#Notes:
#`maxN=` is the number of Ns (dada2 requires none)
#`maxEE=` is the number of expected error allowed in a read. I chose 2, which is how many the tutorial selected.

### Learn the error rates
errF <- learnErrors(filtFs, multithread = TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errF, nominalQ = TRUE) #Visualize the error rates
#Notes:
#The estimated error rates (black lines) generated by the model should fit very close to the observed rates (black points).
#Error drops with increased quality score as expected.

### Dereplication (Note: This is no longer needed for DADA2 current versions as of 10/15/2019)
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
# Name the derep-class objects by the sample names
names(derepFs) <- sample.names
names(derepRs) <- sample.names

### Sample Inference using the core DADA2 algorithm
dadaFs <- dada(derepFs, err = errF, multithread = TRUE)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE)
dadaFs[[10]] #check out how many amplicon sequence variants (ASVs) for a certain sample were found. 

### Merge Paired-end reads
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)

### Construct ASV table
seqtab <- makeSequenceTable(mergers)
dim(seqtab) #should have 66 samples if using the SRA uploaded samples

### Inspect distribution of sequence lengths and trim
table(nchar(getSequences(seqtab)))
#remove sequences that are above 256 and below 250
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% seq(250,256)]
table(nchar(getSequences(seqtab2)))

### Remove chimeras 
seqtab2.nochim <- removeBimeraDenovo(seqtab2, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab2.nochim)
sum(seqtab2.nochim)/sum(seqtab2) #inspect the number of sequences that were kept.

### Track reads through the pipeline to verify no step drastically reduced the read count.
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab2.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track[1:20,] #take a look at the top 20

### Assign taxonomy 
# 100% perfect sequence identity is best according to Edgar 2018 "Updating the 97% identity threshold for 16S robosomal RNA OTUs" doi.org/10.1093/bioinformatics/bty113
# Begin by assigning the overall Taxonomy, then move to assigning the species
# Note: Downloaded the training and species databases that are DADA2 compatible from: https://zenodo.org/record/1172783#.XKUK6etKjVs 
taxa <- assignTaxonomy(seqtab2.nochim, "~/Documents/SequencingData/LibraryY_TideExpt/fastq_files_all/taxonomy/silva_nr_v132_train_set.fa.gz", multithread = TRUE, minBoot = 50) #start with the default bootstrap of 50, but you can go up to 80 if you want. Adjust the path as needed for location of training set.
taxa <- addSpecies(taxa, "~/Documents/SequencingData/LibraryY_TideExpt/fastq_files_all/taxonomy/silva_species_assignment_v132.fa.gz", verbose = TRUE, allowMultiple = 3)#only let there be 3 multiple assignments, and if not, it writes NA. Adjust path to location of training set.

### Recap and write out files:
# This pipeline outputs a taxonomy file: "taxa"
# This pipeline outputs an OTU table: "seqtab2.nochim"
otus <- seqtab2.nochim
taxonomy <- taxa

idx <- match(rownames(taxonomy), colnames(otus)) #looks like they were all aligned, but doesn't hurt just to make it very safe.
otus <- otus[,idx]

#save a dataframe with a new ASV identifier and the sequence from the rownames for taxa
#This is very important because you want to save the sequence for reproducibility and tractability
ASVseqs <- data.frame("asv" = paste0("ASV", seq(from = 1, to = ncol(otus), by = 1)), "sequence" = rownames(taxa))

#rename otu and taxa dataframe so they are easier to interpret
colnames(otus) <- ASVseqs$asv
otus <- t(otus) #change OTU table so that the ASVs are rows and samples are columns
rownames(taxonomy) <- ASVseqs$asv

#write out these as .txt files. These output files are included in this GitHub Repository.
write.table(otus, file = "ASV_counts_USVItide.txt", sep = "\t", row.names = TRUE, col.names = TRUE)
write.table(taxonomy, file = "taxonomy_USVItide.txt", sep = "\t", row.names = TRUE, col.names = TRUE)
write.table(ASVseqs, file = "ASVsequences.txt", sep = "\t", row.names = TRUE, col.names = TRUE)

